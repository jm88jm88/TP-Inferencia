---
title: "Trabajo Práctico 2: Ley de los Grandes Números y Teorema Central del Límite"
subtitle: "Juan Merhle, Juan Martin Goyeneche, Luciano Pozzoli"
output: pdf_document
date: "2025-08-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ejercicio 1

Sea \( X \sim \text{U}(0,18) \) una variable aleatoria con distribución uniforme entre 0 y 18.  
Cuando queremos calcular el valor teorico, tanto de la esperanza como de la varianza en una variable continua, debemos hacer uso de la integral.
En las variables aleatorias discretas tenemos un conjunto finito de valores de los cuales podemos hacer una suma ponderada, pero este no es el caso para las variables aleatorias continuas. Entonces, recurrimos al uso de la integral. En esta usamos los infinitos valores que puede tomar la variable aleatoria, representados con \( x \) y hacemos la ponderacion multiplicandolos por su funcion de densidad de probabilidad \( f_X(x) \).

Entonces, la esperanza se calcula con:
\( \mathbb{E}(X) = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx \)

Pero, nuestra funcion de probabilidad es distinta de cero solo cuando x esta entre 0 y 18. Ademas, sabemos como esta definida la funcion de probabilidad de la uniforme. Entonces, la integral para esta funcion especifica nos quedaria de la siguiente forma:
\[
\mathbb{E}(X) = \int_{a}^{b} x \cdot f_X(x) \, dx = \int_{0}^{18} x \cdot \frac{1}{18} \, dx
\]


Resolvemos la integral:
\[
\int_0^{18} x \, dx = \left. \frac{x^2}{2} \right|_0^{18} = \frac{18^2}{2} - \frac{0^2}{2} = \frac{324}{2} = 162
\]

Entonces:
\[
\mathbb{E}(X) = \frac{1}{18} \cdot 162 = 9
\]

Por lo tanto, la esperanza teórica es:
\[
\boxed{\mathbb{E}(X) = 9}
\]

En cuanto a la varianza, sabemos que el valor teorico tiene la siguiente forma:
\[
\text{Var}(X) = \mathbb{E}(X^2) - \left( \mathbb{E}(X) \right)^2
\]

Afortunadamente ya contamos con el valor de la esperanza y podemos elevarlo al cuadrado sin problema. Lo que si nos resta hacer es el calculo del primer termino de la resta. Para esto tenemos que volver a la integral con la cual resolvemos la esperanza pero utilizar el cuadrado de x en lugar de x. Entonces:
\[
\mathbb{E}(X^2) = \int_{0}^{18} x^2 \cdot f_X(x) \, dx = \int_{0}^{18} x^2 \cdot \frac{1}{18} \, dx
\]

Sacamos la constante \( \frac{1}{18} \) fuera de la integral:
\[
\mathbb{E}(X^2) = \frac{1}{18} \int_{0}^{18} x^2 \, dx
\]

Calculamos la integral:
\[
\int_{0}^{18} x^2 \, dx = \left. \frac{x^3}{3} \right|_0^{18} = \frac{18^3}{3} - \frac{0^3}{3} = \frac{5832}{3} = 1944
\]

Entonces:
\[
\mathbb{E}(X^2) = \frac{1}{18} \cdot 1944 = 108
\]

\[
\mathbb{E}(X^2) = 108
\]

Teniendo ya todos los valores pertinentes mostramos que:
\[
\text{Var}(X) = \mathbb{E}(X^2) - \left( \mathbb{E}(X) \right)^2 = 108 - 81 = 27
\]

Por lo tanto, la varianza teórica es:
\[
\boxed{\text{Var}(X) = 27}
\]

### Inciso B
```{r}
set.seed(13)
X_dist <- function(R){
  return(runif(n= R, min= 0, max=18))
}
```


## (1.c) Histogramas con R = 100 y R = 10^4


```{r}
R100  <- X_dist(100)
R1e4  <- X_dist(10^4)

summary(R100)
summary(R1e4)


par(mfrow = c(1,2))
hist(R100, breaks = 30, main = "X, R = 100", xlab = "x")
hist(R1e4, breaks = 30, main = "X, R = 10000", xlab = "x")
par(mfrow = c(1,1))


```

La Ley de los Grandes Números nos dice que a medida que el tamaño de la muestra (n) aumente, la diferencia entre la media muestral y la media poblacional disminuye. En otras palabras, con un mayor n en la muestra deberiamos observar que la media muestral se acerca a la esperanza.

En nuestro caso la cantidad de valores que generan las muestras aleatorias estan definidas por el parametro R. Y queremos observar que a mayor R la media muestral se aproxima al valor teorico de la esperanza que calculamos anteriormente. Los resultados obtenidos fueron:

\[
\overline{X}_{100}= `r mean(R100)` \quad
\overline{X}_{10000} = `r mean(R1e4)`
\]

Vemos que en estas observaciones se cumple que al aumentar n estamos aproximandonos al valor teorico de \( \mathbb{E}(X) = 9\). La diferencia de los promedios muestrales con la esperanza teorica es casi 10 veces menor cuando aumentamos de R=100 a R=10000. Si vieramos los graficos y juzgaramos por intuicion, tambien coincidiriamos que R=10000 es mas parecido a una distribución Uniforme.

De la misma forma, esta tendencia tambien puede observarse en la varianza. Veamos si a medida que el tamaño de la muestra aumenta, se estabiliza la varianza muestral. Los datos que observamos de las varianzas muestrales fueron:

\[
\text{Var}(X_{100}) = `r var(R100)` \quad
\text{Var}(X_{10000}) = `r var(R1e4)`
\]

En este caso la convergencia de las varianzas muestrales hacia su valor teorico de 27 es muy evidente. A medida que aumentamos el tamaño de la muestra la dispersion se reduce y se asemeja a la varianza de la distribución.

Cabe destacar que estas muestras de datos se realizan con una sola iteración para cada tamaño, y los datos generados son aleatorios.


## (1.d) Promedios acumulados y Ley de los Grandes Números

Construimos los promedios acumulados:

\[
m_n = \frac{1}{n}\sum_{i=1}^n X_i, \quad n=1,2,\dots,10^4
\]

y los comparamos con el valor teórico \(E(X)=9\).

```{r, fig.width=7, fig.height=5}
R <- 10^4
manyX <- X_dist(R)

m_n <- cumsum(manyX) / seq_along(manyX)

plot(seq_along(m_n), m_n, type = "l", lwd = 1,
     xlab = "n", ylab = "Promedio m_n",
     main = "Promedios acumulados (X ~ U(0,18))")
abline(h = 9, col = "red", lty = 2)

# bandas de referencia +/- 2*sd/sqrt(n)
sigma2 <- 27
sd_seq <- sqrt(sigma2 / seq_along(m_n))
lines(seq_along(m_n), 9 + 2*sd_seq, lty = 3)
lines(seq_along(m_n), 9 - 2*sd_seq, lty = 3)

legend("bottomright",
       legend = c("m_n", "E(X)=9", "+/- 2*sd/sqrt(n)"),
       lty = c(1,2,3), bty = "n")

```

## (2.a) Esperanza de \(Y \sim \mathrm{Exp}(\lambda)\) con \(\lambda = \tfrac{1}{9}\)

La densidad es \(f_Y(y) = \lambda e^{-\lambda y}\) para \(y \ge 0\). Entonces,
\[
\mathbb{E}(Y) \;=\; \int_{0}^{\infty} y \,\lambda e^{-\lambda y}\,dy
\]
Para \(\lambda=\tfrac{1}{9}\),
\[
\mathbb{E}(Y) \;=\; \frac{1}{9}\int_{0}^{\infty} y\, e^{-\frac{1}{9}y}\,dy
\]

**Integración por partes.** Tomamos \(u=y \Rightarrow du=dy\) y \(dv=e^{-\frac{1}{9}y}dy \Rightarrow v=-9\,e^{-\frac{1}{9}y}\).
\[
\int_{0}^{\infty} y\, e^{-\frac{1}{9}y}\,dy
= \Big[-9y\,e^{-\frac{1}{9}y}\Big]_{0}^{\infty} + \int_{0}^{\infty} 9\,e^{-\frac{1}{9}y}\,dy
\]
El primer término es \(0\) (porque \(y e^{-\frac{1}{9}y}\to 0\) al \(+\infty\) y en \(0\) vale \(0\)). Para la integral restante:
\[
\int_{0}^{\infty} 9\,e^{-\frac{1}{9}y}\,dy
= 9 \left[\,-9\,e^{-\frac{1}{9}y}\,\right]_{0}^{\infty}
= 9\,(0 - (-9)) = 81
\]
Por lo tanto,
\[
\mathbb{E}(Y) \;=\; \frac{1}{9}\cdot 81 \;=\; 9.
\]
\[
\boxed{\mathbb{E}(Y)=9}
\]

> Observación: en general, si \(Y\sim \mathrm{Exp}(\lambda)\), entonces \(\mathbb{E}(Y)=\frac{1}{\lambda}\) y \(\mathrm{Var}(Y)=\frac{1}{\lambda^2}\). Para \(\lambda=\tfrac{1}{9}\), \(\mathrm{Var}(Y)=81\).




Teniendo el valor teorico de la esperanza, recordamos que la formula de la varianza es:
\[
\text{Var}(Y) = \mathbb{E}(Y^2) - \left( \mathbb{E}(Y) \right)^2
\]

El calculo anterior nos sirve dado que nos resuelve uno de los terminos ya que hacer el cuadrado de la esperanza es una cuenta fácil, pero si necesitamos volver a hacer la integral con el cuadrado de y para poder resolver la primera parte de la resta.

La esperanza de \( Y^2 \) se calcula como:
\[
\mathbb{E}(Y^2) = \int_0^{\infty} y^2 \cdot \frac{1}{9} e^{-\frac{1}{9} y} \, dy
\]

Sacamos la constante fuera de la integral:
\[
\mathbb{E}(Y^2) = \frac{1}{9} \int_0^{\infty} y^2 \cdot e^{-\frac{1}{9} y} \, dy
\]

Ahora resolvemos la integral por partes dos veces.

Primera integración por partes:  
Sea  
\[
u = y^2 \quad \Rightarrow \quad du = 2y \, dy \\
dv = e^{-\frac{1}{9} y} dy \quad \Rightarrow \quad v = -9 e^{-\frac{1}{9} y}
\]

Aplicamos:
\[
\int y^2 \cdot e^{-\frac{1}{9} y} dy = -9 y^2 e^{-\frac{1}{9} y} + \int 18y \cdot e^{-\frac{1}{9} y} dy
\]

Ahora resolvemos la segunda integral:  
\[
\int 18y \cdot e^{-\frac{1}{9} y} dy
\]

Segunda integración por partes:
\[
u = y \quad \Rightarrow \quad du = dy \\
dv = e^{-\frac{1}{9} y} dy \quad \Rightarrow \quad v = -9 e^{-\frac{1}{9} y}
\]

Entonces:
\[
\int 18y \cdot e^{-\frac{1}{9} y} dy = 18 \left( -9y e^{-\frac{1}{9} y} + \int 9 e^{-\frac{1}{9} y} dy \right)
\]

Resolvemos la última integral:
\[
\int 9 e^{-\frac{1}{9} y} dy = -81 e^{-\frac{1}{9} y}
\]

Sumamos todo:
\[
\int y^2 e^{-\frac{1}{9} y} dy = -9 y^2 e^{-\frac{1}{9} y} + 18 \left( -9y e^{-\frac{1}{9} y} - 81 e^{-\frac{1}{9} y} \right)
\]

Multiplicamos y reordenamos:
\[
= -9 y^2 e^{-\frac{1}{9} y} - 162 y e^{-\frac{1}{9} y} - 1458 e^{-\frac{1}{9} y}
\]

Evaluamos en los límites \( 0 \) a \( \infty \). Todos los términos tienden a 0 en \( y \to \infty \), y en \( y = 0 \) queda:
\[
-0 - 0 - 1458 \cdot 1 = -1458
\]

Entonces la integral completa da 1458, y por lo tanto:
\[
\mathbb{E}(Y^2) = \frac{1}{9} \cdot 1458 = 162
\]

Resultado final:
\[
\mathbb{E}(Y^2) = 162
\]


Teniendo ya todos los terminos necesarios, procedemos a hacer el calculo teorico de la varianza.
\[
\text{Var}(Y) = \mathbb{E}(Y^2) - \left( \mathbb{E}(Y) \right)^2 = 162 - 81 = 81
\]
\[
\boxed{\text{Var}(Y) = 81}
\]

## (2.b) Función generadora de muestras

Definimos la función generadora de muestras \(Y\_dist(R)\) que devuelve \(R\) realizaciones de \(Y\). Usamos nuestra semilla de grupo (13) para asegurar reproducibilidad.

```{r}
set.seed(13)

Y_dist <- function(R) {
  rexp(n = R, rate = 1/9)
}
```

## (2.c) Histogramas con R = 100 y R = 10^4


```{r}
YR100  <- Y_dist(100)
YR1e4  <- Y_dist(10^4)

summary(YR100)
summary(YR1e4)


par(mfrow = c(1,2))
hist(YR100, breaks = 30, main = "Y, R = 100", xlab = "x")
hist(YR1e4, breaks = 30, main = "Y, R = 10000", xlab = "x")
par(mfrow = c(1,1))
```

En el caso de la media, podemos notar que a medida que aumentamos R, obtenemos valores cada vez mas aproximados a la esperanza teorica de valor 9. Esta observacion se sustenta con lo que dice la Ley de lo Grandes Numeros. Los valores obtenidos de las medias muestrales son:
\[
\overline{Y}_{100} = `r mean(YR100)` \quad
\overline{Y}_{10000} = `r mean(YR1e4)`
\]
En este caso podemos ver que la convergencia se cumple y queda en evidencia que la diferencia entre E(Y)=9 y las medias muestrales disminuye a medida que aumenta el tamaño de la muestra.

Lo mismo podemos observar con los valores de la varianza. Al aumentar el n notamos un acercamiento muy grande de la varianza muestral al valor de la varianza poblacional. Recordando que Var(Y) = 81:
\[
\text{Var}(Y_{100}) = `r var(YR100)` \quad
\text{Var}(Y_{10000}) = `r var(YR1e4)`
\]

---

## Punto 3: Promedios de variables aleatorias

Sea \(X \sim U(0,18)\). Definimos:

\[
X_{15} = \frac{1}{15}\sum_{i=1}^{15} X_i
\]

donde los \(X_i\) son 15 variables independientes con la misma distribución que \(X\).

### (3.a) Esperanza y varianza de \(X_{15}\)

Usamos propiedades lineales:

\[
E(X_{15}) = \frac{1}{15}\sum_{i=1}^{15}E(X_i) = \frac{1}{15}\cdot 15\cdot E(X) = E(X) = 9
\]

\[
Var(X_{15}) = \frac{1}{15^2}\sum_{i=1}^{15}Var(X_i) = \frac{1}{225}\cdot 15\cdot Var(X) = \frac{Var(X)}{15}
\]

Como \(Var(X)=27\):

\[
Var(X_{15}) = \frac{27}{15} = 1.8
\]

**Comentarios:**  
- La esperanza sigue siendo 9.  
- La varianza disminuye al dividir por \(n\), pasa de 27 a 1.8.  
- En general, promediar reduce la dispersión de los valores.

### (3.b) Función `Xn_dist(n, R)`

Definimos una función que genere \(R\) realizaciones del promedio de \(n\) variables \(X\):

```{r}
Xn_dist <- function(n, R) {
  replicate(R, mean(runif(n, min = 0, max = 18)))
}
```

### (3.c) Histograma de X15

```{r}
R <- 10^4
X15 <- Xn_dist(15, R)

hist(X15, breaks = 30, probability = TRUE,
     main = expression("Histograma de " ~ X[15]),
     xlab = "Valor", col = "lightblue", border = "gray")

abline(v = mean(X15), col = "red", lwd = 2, lty = 2)
```

**Comentarios:**

-El histograma de X15 ya no es uniforme como el de X. Su forma es más acampanada y concentrada alrededor de la media (9).

-Esto se debe a que al promediar 15 valores independientes, se reduce la variabilidad y la distribución se aproxima a una normal.

### (4.a) Histogramas de X, X_{40}, Y, Y_{40} con curvas teóricas.

En este inciso aplicamos el Teorema Central del Límite a \(X\sim U(0,18)\) y \(Y\sim \text{Exp}(1/9)\).  
Primero mostramos la forma de las distribuciones originales y luego la de los promedios de 40 observaciones.  
Superponemos las curvas teóricas para ver cómo aparecen las aproximaciones normales.

```{r}
set.seed(13)

R_big <- 10^6
n <- 40

#Usamos funciones definidas antes
X <- X_dist(R_big)
Y <- Y_dist(R_big)

#Promedios de n=40
X40 <- replicate(R_big, mean(X_dist(n)))
Y40 <- replicate(R_big, mean(Y_dist(n)))

#Parámetros teóricos
mu_X <- 9
var_X <- 27
mu_Y <- 9
var_Y <- 81

#Gráficos
par(mfrow = c(2,2))

hist(X, breaks = 60, freq = FALSE, main = X ~ U(0,18))
curve(dunif(x,0,18), add = TRUE, col="red", lwd=2)

hist(X40, breaks = 60, freq = FALSE, main = expression(X[40]))
curve(dnorm(x, mean=mu_X, sd=sqrt(var_X/n)), add = TRUE, col="blue", lwd=2)

hist(Y, breaks = 60, freq = FALSE, main = expression(Y %~% Exp(1/9)))
curve(dexp(x, rate=1/9), add = TRUE, col="red", lwd=2)

hist(Y40, breaks = 60, freq = FALSE, main = expression(Y[40]))
curve(dnorm(x, mean=mu_Y, sd=sqrt(var_Y/n)), add = TRUE, col="blue", lwd=2)

par(mfrow = c(1,1))

```

### 4(b) — TCL con \(X_n\): efecto de \(n\) y \(R\)

Ahora estudiamos cómo varía la distribución de \(X_n\) (el promedio de \(n\) uniformes) cuando cambiamos \(n\) y el número de simulaciones \(R\).  
El panel 5×2 nos deja comparar la forma de los histogramas y el ajuste a la normal según el tamaño de la muestra y el tamaño de la simulación.
```{r}
set.seed(13)

Xn_dist <- function(n, R) replicate(R, mean(X_dist(n)))

par(
  mfrow = c(5, 2),
  mar   = c(2, 2, 2, 1))

n_vals <- c(1, 2, 5, 15, 40)
R_vals <- c(1e2, 1e6)


Xn <- replicate(10^2, mean(X_dist(1)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[1] * ", R=100"), xlab = "", ylab = "",
     ylim = c(0, 0.2))
curve(dnorm(x, mean = 9, sd = sqrt(27/1)), add = TRUE, lwd = 2)

Xn <- replicate(10^6, mean(X_dist(1)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[1] * ", R=10"^6), xlab = "", ylab = "",
     ylim = c(0, 0.2))
curve(dnorm(x, mean = 9, sd = sqrt(27/1)), add = TRUE, lwd = 2)

Xn <- replicate(10^2, mean(X_dist(2)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[2] * ", R=100"), xlab = "", ylab = "",
     ylim = c(0, 0.2))
curve(dnorm(x, mean = 9, sd = sqrt(27/2)), add = TRUE, lwd = 2)

Xn <- replicate(10^6, mean(X_dist(2)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[2] * ", R=10"^6), xlab = "", ylab = "",
     ylim = c(0, 0.2))
curve(dnorm(x, mean = 9, sd = sqrt(27/2)), add = TRUE, lwd = 2)

Xn <- replicate(10^2, mean(X_dist(5)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[5] * ", R=100"), xlab = "", ylab = "",
     ylim = c(0, 0.25))
curve(dnorm(x, mean = 9, sd = sqrt(27/5)), add = TRUE, lwd = 2)

Xn <- replicate(10^6, mean(X_dist(5)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[5] * ", R=10"^6), xlab = "", ylab = "",
     ylim = c(0, 0.25))
curve(dnorm(x, mean = 9, sd = sqrt(27/5)), add = TRUE, lwd = 2)

Xn <- replicate(10^2, mean(X_dist(15)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[15] * ", R=100"), xlab = "", ylab = "",
     ylim = c(0, 0.6))
curve(dnorm(x, mean = 9, sd = sqrt(27/15)), add = TRUE, lwd = 2)

Xn <- replicate(10^6, mean(X_dist(15)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[15] * ", R=10"^6), xlab = "", ylab = "",
     ylim = c(0, 0.6))
curve(dnorm(x, mean = 9, sd = sqrt(27/15)), add = TRUE, lwd = 2)

Xn <- replicate(10^2, mean(X_dist(40)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[40] * ", R=100"), xlab = "", ylab = "",
     ylim = c(0, 0.6))
curve(dnorm(x, mean = 9, sd = sqrt(27/40)), add = TRUE, lwd = 2)

Xn <- replicate(10^6, mean(X_dist(40)))
hist(Xn, breaks = 60, probability = TRUE,
     main = expression(X[40] * ", R=10"^6), xlab = "", ylab = "",
     ylim = c(0, 0.6))
curve(dnorm(x, mean = 9, sd = sqrt(27/40)), add = TRUE, lwd = 2)

par(mfrow = c(1,1))



```
**Comentarios (Xn):**

- **n = 1:** uniforme en [0,18], media = 9. Con R grande se ve perfectamente rectangular.  
- **n = 2:** distribución triangular, ya se “redondea” en el centro.  
- **n = 5:** comienza a verse simétrica y con forma de campana; la normal se ajusta mejor.  
- **n = 15:** claramente normal, muy concentrada alrededor de 9; varianza mucho menor.  
- **n = 40:** se ajusta muy bien a la normal teórica \( \mathcal{N}(9, 27/40) \), extremos casi nulos. 
- **se puede concluir:** como dice el TCL, a medida que la muestra sea lo suficientemente grande, la media de la muestra tiene distribucion aproximadamente normal

**Efecto de R:** más simulaciones → histograma más suave.


## 4(c) — TCL con \(Y_n\): efecto de \(n\) y \(R\)

Finalmente repetimos el análisis para \(Y_n\) (promedios de exponenciales).  
Así podemos discutir cómo la convergencia hacia la normal es más lenta en una distribución asimétrica: hacen falta valores de \(n\) más grandes para que la forma se parezca a una campana.
```{r}
set.seed(13)

# función general: devuelve R realizaciones del promedio de n muestras de Y
Yn_dist <- function(n, R) {
  replicate(R, mean(Y_dist(n)))
}

n_vals <- c(1, 2, 5, 15, 40)
R_vals <- c(10^2, 10^6)

par(mfrow = c(5,2), mar= c(2, 2, 2, 1))
for (n in n_vals) {
  for (R in R_vals) {
    Yn <- Yn_dist(n, R)
    hist(Yn, breaks = 50, freq = FALSE,
         main = bquote(Y[.(n)] ~ ", R=" ~ .(format(R, scientific=FALSE))),
         xlab = "", ylab = "",
         ylim = c(0, 0.3))
    
    #Superponemos la normal del TCL
    mu <- 9
    sd <- sqrt(81 / n)   #varianza de Y es 81
    curve(dnorm(x, mean = mu, sd = sd), add = TRUE, col="blue", lwd=2)
  }
}
par(mfrow = c(1,1))
```
**Comentarios (Yn):**

- **n = 1:** distribución exponencial pura, muy asimétrica a la derecha, media = 9.  
- **n = 2:** sigue siendo claramente asimétrica, pero empieza a concentrarse alrededor de 9.  
- **n = 5:** la media se mantiene en 9, la varianza ya es menor (\(81/5\)), pero todavía hay cola a la derecha.  
- **n = 15:** la forma empieza a parecer normal; la asimetría se atenúa bastante.  
- **n = 40:** se observa un buen ajuste a la normal \(\mathcal{N}(9, 81/40)\), la cola derecha es mucho menos notoria.  

**Comparación con Xn:**  
- En ambos casos la media se mantiene en 9 y la varianza decrece como \(1/n\).  
- La diferencia está en la **velocidad de convergencia**: para la uniforme, con \(n=5\) ya se veía muy normal; en la exponencial, recién con \(n=15\) o más se aproxima bien a la campana.

**Conclusion Ejercicio 4**
Tanto para la uniforme como para la exponencial, los promedios mantienen la misma media que la distribución original y su varianza disminuye proporcionalmente a \(1/n\).  
En el caso de \(X_n\) (uniforme), la convergencia hacia la normal es rápida: con valores de \(n\) moderados ya se observa un buen ajuste.  
En el caso de \(Y_n\) (exponencial), la asimetría inicial hace que la convergencia sea más lenta, necesitando \(n\) mayores para que la forma se aproxime bien a una normal.  
En conjunto, los resultados ilustran con claridad cómo el TCL opera de manera general, aunque la velocidad del ajuste depende de la distribución original.
